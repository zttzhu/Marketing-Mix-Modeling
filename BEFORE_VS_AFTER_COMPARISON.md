# Before vs After: Baseline Model Comparison

## Visual Comparison

### OLD MODEL (RÂ² = -0.245) âŒ
```
[Week 1] â†’ TV Spend: $100k â†’ Immediate Impact: $100k â†’ Sales Effect
[Week 2] â†’ TV Spend: $50k  â†’ Immediate Impact: $50k  â†’ Sales Effect
[Week 3] â†’ TV Spend: $0    â†’ Immediate Impact: $0    â†’ Sales Effect
                              
Only 3 media channels used
Only 2 control variables used
No carryover effects
Linear relationships only
No regularization
```

**Result:** Model can't explain sales spikes â†’ Predicts flat line â†’ RÂ² is NEGATIVE

---

### NEW MODEL (Expected RÂ² = 0.60-0.80) âœ…
```
[Week 1] â†’ TV Spend: $100k â†’ Adstocked: $100k        â†’ Sales Effect
[Week 2] â†’ TV Spend: $50k  â†’ Adstocked: $100k+$50k   â†’ Sales Effect (carryover!)
[Week 3] â†’ TV Spend: $0    â†’ Adstocked: $75k          â†’ Sales Effect (decay)

All 13 media channels used
All 30+ control variables used (holidays, seasonality, etc.)
Adstock transformation (carryover effects)
Feature scaling + Ridge regularization
```

**Result:** Model captures complex patterns â†’ Follows actual trends â†’ RÂ² is POSITIVE

---

## Side-by-Side Feature Comparison

| Aspect | OLD Model | NEW Model |
|--------|-----------|-----------|
| **Media Channels** | 3 channels (mdsp_dm, mdsp_inst, mdsp_nsp) | All 13 channels |
| **Control Variables** | 2 controls (me_ics_all, me_gas_dpg) | All ~30 controls |
| **Adstock** | âŒ None | âœ… Geometric decay (0.5) |
| **Saturation** | âŒ None | Not yet (can add) |
| **Scaling** | âŒ None | âœ… StandardScaler |
| **Regularization** | âŒ None (OLS) | âœ… Ridge (alpha=100) |
| **Log Transform** | âœ… Yes | âŒ No (using raw) |
| **Features Used** | 5 total | 43+ total |
| **Model Type** | statsmodels OLS | sklearn Ridge |

---

## What Changed in the Code

### 1. Variable Selection
```python
# OLD (BAD)
media_cols_baseline = mdsp_col[:3]  # Only first 3
control_cols_baseline = base_vars[:2]  # Only first 2

# NEW (GOOD)
media_cols_baseline = mdsp_col  # ALL media channels
control_cols_baseline = base_vars  # ALL control variables
```

### 2. Adstock Transformation
```python
# NEW: Add this function
def geometric_adstock_simple(x, decay=0.5):
    adstocked = np.zeros_like(x, dtype=float)
    adstocked[0] = x[0]
    for i in range(1, len(x)):
        adstocked[i] = x[i] + decay * adstocked[i-1]
    return adstocked

# Apply to all media
for col in media_cols_baseline:
    model_data[f'{col}_adstock'] = geometric_adstock_simple(
        model_data[col].values, decay=0.5
    )
```

### 3. Feature Scaling
```python
# NEW: Scale features before fitting
scaler_baseline = StandardScaler()
X_train_scaled = scaler_baseline.fit_transform(X_train)
X_test_scaled = scaler_baseline.transform(X_test)
```

### 4. Ridge Regression
```python
# OLD (BAD)
baseline_ols_model = sm.OLS(y_train, X_train).fit()

# NEW (GOOD)
baseline_ridge_model = Ridge(alpha=100.0)
baseline_ridge_model.fit(X_train_scaled, y_train)
```

---

## How Adstock Works (Example)

### Scenario: TV Ad Campaign
- **Week 1:** Spend $1M on TV ads
- **Week 2:** Spend $0 (no ads)
- **Week 3:** Spend $0 (no ads)

### Without Adstock (OLD MODEL):
```
Week 1 Sales Effect: $1M Ã— coefficient = impact
Week 2 Sales Effect: $0 Ã— coefficient = 0 impact âŒ WRONG!
Week 3 Sales Effect: $0 Ã— coefficient = 0 impact âŒ WRONG!
```
**Problem:** Assumes ad impact ends immediately when spending stops.

### With Adstock (NEW MODEL, decay=0.5):
```
Week 1 Adstocked Value: $1M
Week 2 Adstocked Value: $1M Ã— 0.5 = $500k (carryover effect!)
Week 3 Adstocked Value: $500k Ã— 0.5 = $250k (still some effect!)

Week 1 Sales Effect: $1M Ã— coefficient = impact
Week 2 Sales Effect: $500k Ã— coefficient = continued impact âœ… CORRECT!
Week 3 Sales Effect: $250k Ã— coefficient = decaying impact âœ… CORRECT!
```
**Benefit:** Captures realistic advertising decay over time.

---

## Why This Matters for Your Data

Looking at your chart, the actual sales show:
- **Huge spikes** (up to 3.5e8) â†’ likely holiday periods or major promotions
- **Strong seasonality** â†’ year-end peaks
- **High volatility** â†’ week-to-week changes

### OLD Model Couldn't Capture This Because:
1. Missing holiday indicators (only used 2 controls)
2. Missing seasonal variables (seas_prd_*, seas_week_*)
3. No adstock (couldn't explain why sales stayed high for weeks after big media push)
4. Only 3 media channels (missing 10 other important drivers)

### NEW Model Can Capture This Because:
1. âœ… **Has ALL holiday indicators** (hldy_Black Friday, hldy_Christmas, etc.)
2. âœ… **Has ALL seasonal variables** (seas_prd_1-12, seas_week_40-48)
3. âœ… **Adstock explains sustained effects** after media spends
4. âœ… **All 13 media channels** included
5. âœ… **Discount/promotion variables** (mrkdn_*, va_pub_*)

---

## Expected Visual Improvement

### Before (RÂ² = -0.245):
```
Actual:    /\    /\        /\
          /  \  /  \  /\  /  \___
         /    \/    \/  \/
        
Predicted: _________________ (flat line at mean)
```

### After (Expected RÂ² = 0.60-0.80):
```
Actual:    /\    /\        /\
          /  \  /  \  /\  /  \___
         /    \/    \/  \/
        
Predicted: /\   /\       /\
          /  \_/  \  /\_/  \___
         /        \/
         
(Following the actual trends!)
```

---

## Diagnostic Metrics to Watch

When you run the improved model, you should see:

### âœ… **Good Signs:**
- RÂ² test > 0.6
- RÂ² train â‰ˆ RÂ² test (within 0.1-0.15)
- MAPE < 20%
- Residuals randomly scattered (no pattern)
- Top features make business sense

### âš ï¸ **Warning Signs:**
- RÂ² train >> RÂ² test (e.g., 0.9 vs 0.5) â†’ Overfitting
- RÂ² still negative â†’ Something fundamentally wrong
- All predictions very similar â†’ Model not learning
- Top features are all noise variables â†’ Bad model

### ğŸ”´ **Red Flags:**
- RÂ² test < 0 â†’ Model worse than baseline
- MAPE > 50% â†’ Predictions way off
- Feature coefficients are huge â†’ Scaling issue
- Predictions outside data range â†’ Extrapolation problem

---

## Testing the Improvements

Run this and compare:

```python
# After running mmm_script.py, check:

1. Console output for "IMPROVED BASELINE MODEL RESULTS"
   - Is RÂ² positive?
   - Is RÂ² > 0.6?

2. Chart: "Test Set: Improved Baseline Model"
   - Does predicted line follow actual trends?
   - Are peaks/valleys aligned?

3. Residual plot:
   - Points randomly scattered? âœ… Good
   - Clear pattern visible? âŒ Bad

4. Top 10 features:
   - Do they make business sense?
   - Are important media channels included?
```

---

## Quick Wins if RÂ² is Still Low

If RÂ² < 0.5 after improvements:

1. **Tune adstock decay:**
   ```python
   # Try decay values: 0.3, 0.5, 0.7, 0.9
   # Higher decay = longer carryover
   ```

2. **Tune Ridge alpha:**
   ```python
   # Try alpha values: 10, 50, 100, 500, 1000
   # Higher alpha = more regularization
   ```

3. **Check for outliers:**
   ```python
   # Remove extreme sales values?
   # Or use log transformation?
   ```

4. **Add polynomial features:**
   ```python
   from sklearn.preprocessing import PolynomialFeatures
   poly = PolynomialFeatures(degree=2, interaction_only=True)
   ```

5. **Try different model:**
   ```python
   from sklearn.ensemble import RandomForestRegressor
   # Or XGBoost, LightGBM
   ```

---

## Summary

**What We Fixed:**
1. âœ… Using ALL variables (not just 5)
2. âœ… Adding adstock transformation
3. âœ… Feature scaling
4. âœ… Ridge regularization

**Expected Outcome:**
- RÂ² improvement from **-0.245** to **0.60-0.80** (+0.85 to +1.05 gain)
- Predictions that actually follow sales trends
- Meaningful feature importance insights

**Next Step:**
Run `python mmm_script.py` and check if RÂ² is now positive and > 0.6! ğŸš€
